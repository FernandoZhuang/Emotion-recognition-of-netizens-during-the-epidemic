{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import my_setting.utils as utils\n",
    "import cufflinks as cf\n",
    "import jieba\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "%matplotlib inline\n",
    "cf.go_offline()\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('max_colwidth',100)\n",
    "plt.style.use('seaborn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>poster</th>\n",
       "      <th>content</th>\n",
       "      <th>image</th>\n",
       "      <th>video</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>﻿更博了，爆照了，帅的呀，就是越来越爱你！生快傻缺[爱你][爱你][爱你]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@张晓鹏jonathan 土耳其的事要认真对待[哈哈]，否则直接开除。@丁丁看世界 很是细心，酒店都全部OK啦。</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>姑娘都羡慕你呢…还有招财猫高兴……//@爱在蔓延-JC:[哈哈]小学徒一枚，等着明天见您呢//@李欣芸SharonLee:大佬范儿[书呆子]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>美~~~~~[爱你]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>梦想有多大，舞台就有多大![鼓掌]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[花心][鼓掌]//@小懒猫Melody2011: [春暖花开]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>某问答社区上收到一大学生发给我的私信：“偶喜欢阿姨！偶是阿姨控！”我回他：“阿姨稀饭小盆友！偶是小盆友控！” [哈哈]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>吃货们无不啧啧称奇，好不喜欢！PS:写错一个字！[哈哈]@森林小天使-波琪 @SEVEN厦门摄影师 @日月星辰-心在路上 @每种型号生两胎 @志远天下行 @监控防盗安装XM @创意美食simo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#Sweet Morning#From now on,love yourself,enjoy living then smile.从现在开始，爱自己，享受生活并且微笑。[呵呵] [嘻嘻] [哈...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>【霍思燕剖腹产下“小江江” 老公落泪】今晨9时霍思燕产下一名男婴，宝宝重8斤3两，母子平安。杜江的脸上洋溢着做爸爸的欣喜：宝宝小名叫“小江江”，眼睛像他，鼻子和嘴巴则像霍思燕，看到宝贝就忍不住...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   datetime  poster  \\\n",
       "0       NaN     NaN   \n",
       "1       NaN     NaN   \n",
       "2       NaN     NaN   \n",
       "3       NaN     NaN   \n",
       "4       NaN     NaN   \n",
       "5       NaN     NaN   \n",
       "6       NaN     NaN   \n",
       "7       NaN     NaN   \n",
       "8       NaN     NaN   \n",
       "9       NaN     NaN   \n",
       "\n",
       "                                                                                               content  \\\n",
       "0                                                                ﻿更博了，爆照了，帅的呀，就是越来越爱你！生快傻缺[爱你][爱你][爱你]   \n",
       "1                                             @张晓鹏jonathan 土耳其的事要认真对待[哈哈]，否则直接开除。@丁丁看世界 很是细心，酒店都全部OK啦。   \n",
       "2                               姑娘都羡慕你呢…还有招财猫高兴……//@爱在蔓延-JC:[哈哈]小学徒一枚，等着明天见您呢//@李欣芸SharonLee:大佬范儿[书呆子]   \n",
       "3                                                                                           美~~~~~[爱你]   \n",
       "4                                                                                    梦想有多大，舞台就有多大![鼓掌]   \n",
       "5                                                                     [花心][鼓掌]//@小懒猫Melody2011: [春暖花开]   \n",
       "6                                          某问答社区上收到一大学生发给我的私信：“偶喜欢阿姨！偶是阿姨控！”我回他：“阿姨稀饭小盆友！偶是小盆友控！” [哈哈]   \n",
       "7  吃货们无不啧啧称奇，好不喜欢！PS:写错一个字！[哈哈]@森林小天使-波琪 @SEVEN厦门摄影师 @日月星辰-心在路上 @每种型号生两胎 @志远天下行 @监控防盗安装XM @创意美食simo...   \n",
       "8  #Sweet Morning#From now on,love yourself,enjoy living then smile.从现在开始，爱自己，享受生活并且微笑。[呵呵] [嘻嘻] [哈...   \n",
       "9  【霍思燕剖腹产下“小江江” 老公落泪】今晨9时霍思燕产下一名男婴，宝宝重8斤3两，母子平安。杜江的脸上洋溢着做爸爸的欣喜：宝宝小名叫“小江江”，眼睛像他，鼻子和嘴巴则像霍思燕，看到宝贝就忍不住...   \n",
       "\n",
       "   image  video  sentiment  \n",
       "0    NaN    NaN          1  \n",
       "1    NaN    NaN          1  \n",
       "2    NaN    NaN          1  \n",
       "3    NaN    NaN          1  \n",
       "4    NaN    NaN          1  \n",
       "5    NaN    NaN          1  \n",
       "6    NaN    NaN          1  \n",
       "7    NaN    NaN          1  \n",
       "8    NaN    NaN          1  \n",
       "9    NaN    NaN          1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplify_weibo=pd.read_csv('/home/zhw/PycharmProjects/nCovSentimentAnalysis/Data/relevent_senti_100k.csv',encoding='utf-8')\n",
    "simplify_weibo.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplify_weibo.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.bar(simplify_weibo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=simplify_weibo['label'].value_counts()\n",
    "temp_df=pd.DataFrame({'labels':temp.index,'values':temp.values})\n",
    "temp_df.iplot(kind='pie',labels='labels',values='values',title='Labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplify_weibo['length']=simplify_weibo['review'].astype(str).apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1=plt.subplots(1,figsize=(14,8))\n",
    "sns.distplot(simplify_weibo['length'],ax=ax1,color='green')\n",
    "plt.rcParams['font.family']='STSong'\n",
    "plt.rcParams['axes.unicode_minus']=False\n",
    "ax1.set_title('长度分布')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nip_corpus=pd.read_csv('/home/zhw/PycharmProjects/nCovSentimentAnalysis/Data/SentimentRelevantCorpus/unzip/chineseNIP_weibo_senti_100k.csv',encoding='utf-8')\n",
    "nip_corpus.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nip_corpus.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "msno.bar(nip_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "temp = nip_corpus['label'].value_counts()\n",
    "temp_df = pd.DataFrame({'labels':temp.index,'values':temp.values})\n",
    "temp_df.iplot(kind='pie',labels='labels',values='values',title='Labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nip_corpus['length']=nip_corpus['review'].astype(str).apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1,figsize=(14,8))\n",
    "sns.distplot(nip_corpus['length'],ax=ax1, color='green')\n",
    "plt.rcParams['font.family']='SIMHEI' \n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "ax1.set_title('训练集微博长度分布')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据\n",
    "\n",
    "由于其他编码会出现少部分汉字乱码，比如“XX超话”、表情符号等。所以我手动用记事本转换成了 UTF-8 编码。\n",
    "\n",
    "其中，超话以及其他特殊字符以“”表示，大部分表情符号被“??”代替。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_labeled = pd.read_csv('/home/zhw/PycharmProjects/nCovSentimentAnalysis/Data/UTF8nCoV_100k_train.labled.csv', encoding='utf-8')\n",
    "train_labeled.rename(columns = {\"微博id\": \"Weibo_ID\",\n",
    "                                \"微博发布时间\": \"Publish_Time\", \n",
    "                                \"发布人账号\": \"Account_ID\",\n",
    "                                \"微博中文内容\": \"Chinese_Content\",\n",
    "                                \"微博图片\": \"Pictures\",\n",
    "                                \"微博视频\": \"Videos\",\n",
    "                                \"情感倾向\": \"Labels\"},  inplace=True)\n",
    "\n",
    "train_labeled_copy = train_labeled.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('/home/zhw/PycharmProjects/nCovSentimentAnalysis/Data/UTF8nCov_10k_test.csv', encoding='utf-8')\n",
    "test.rename(columns = {\"微博id\": \"Weibo_ID\",\n",
    "                                \"微博发布时间\": \"Publish_Time\", \n",
    "                                \"发布人账号\": \"Account_ID\",\n",
    "                                \"微博中文内容\": \"Chinese_Content\",\n",
    "                                \"微博图片\": \"Pictures\",\n",
    "                                \"微博视频\": \"Videos\"},  inplace=True)\n",
    "\n",
    "test_copy = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labeled.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 缺失值检查\n",
    "\n",
    "检查一下训练集和测试集的缺失值，可以看出有些数据是缺失的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labeled.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.bar(train_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.bar(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有少量微博正文数据丢失，部分labels为空。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标签检查\n",
    "\n",
    "标签统计如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labeled_copy.fillna({\"Labels\": \"empty_label\"}, inplace=True)\n",
    "\n",
    "temp = train_labeled_copy[\"Labels\"].value_counts()\n",
    "temp_df = pd.DataFrame({'labels': temp.index,\n",
    "                        'values': temp.values})\n",
    "\n",
    "temp_df.iplot(kind='pie',labels='labels',values='values', title='Labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有6种噪音标签，每种各一个，此外还有一点点空标签，统计如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = train_labeled_copy[(train_labeled_copy[\"Labels\"] != '0') & \n",
    "                           (train_labeled_copy[\"Labels\"] != '1') & \n",
    "                           (train_labeled_copy[\"Labels\"] != '-1')]\n",
    "\n",
    "noise = noise[\"Labels\"].value_counts()\n",
    "noise_df = pd.DataFrame({'labels': noise.index,\n",
    "                         'values': noise.values})\n",
    "\n",
    "noise_df.iplot(kind='pie',labels='labels',values='values', title='奇怪标签统计')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于非正常标签的处理，可以直接舍弃，也可以手工打上真正标签（如果不嫌累）。\n",
    "\n",
    "这里我们直接舍弃就行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labeled_copy['time'] = pd.to_datetime('2020年' + train_labeled['Publish_Time'], format='%Y年%m月%d日 %H:%M', errors='ignore')\n",
    "test_copy['time'] = pd.to_datetime('2020年' + train_labeled['Publish_Time'], format='%Y年%m月%d日 %H:%M', errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labeled_copy['month'] =  train_labeled_copy['time'].dt.month\n",
    "train_labeled_copy['day'] =  train_labeled_copy['time'].dt.day\n",
    "train_labeled_copy['dayfromzero']  = (train_labeled_copy['month'] - 1) * 31 +  train_labeled_copy['day']\n",
    "\n",
    "test_copy['month'] =  test_copy['time'].dt.month\n",
    "test_copy['day'] =  test_copy['time'].dt.day\n",
    "test_copy['dayfromzero']  = (test_copy['month'] - 1) * 31 +  test_copy['day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "sns.kdeplot(train_labeled_copy.loc[train_labeled_copy['Labels'] == '0', 'dayfromzero'], ax=ax[0], label='sent(0)')\n",
    "sns.kdeplot(train_labeled_copy.loc[train_labeled_copy['Labels'] == '1', 'dayfromzero'], ax=ax[0], label='sent(1)')\n",
    "sns.kdeplot(train_labeled_copy.loc[train_labeled_copy['Labels'] == '-1', 'dayfromzero'], ax=ax[0], label='sent(-1)')\n",
    "\n",
    "train_labeled_copy.loc[train_labeled_copy['Labels'] == '0', 'dayfromzero'].hist(ax=ax[1])\n",
    "train_labeled_copy.loc[train_labeled_copy['Labels'] == '1', 'dayfromzero'].hist(ax=ax[1])\n",
    "train_labeled_copy.loc[train_labeled_copy['Labels'] == '-1', 'dayfromzero'].hist(ax=ax[1])\n",
    "ax[1].legend(['sent(0)', 'sent(1)','sent(-1)'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出如下情况，顺便也帮大家找了找相关新闻节点（深藏功与名）：\n",
    "\n",
    "- **1月18日**后，话题量有明显增长。\n",
    "    - 1月19日：武汉CDC李刚：新冠人传人风险较低，传染力不强。\n",
    "    - 1月20日：钟南山:新型肺炎存在人传人现象。\n",
    "    - 1月20日：口罩出现抢购现象。\n",
    "    - 1月23日：武汉封城。\n",
    "    - 1月25日：火神山医院设计方案完成；雷神山医院建造决定。\n",
    "\n",
    "- **二月九日**前后，话题量达到顶峰。\n",
    "    - 2月7日：“吹哨人”李文亮医生不幸去世。\n",
    "    - 2月10日：湖北省相关领导任免。\n",
    "\n",
    "18日前官方口径（包括网友）们的态度还比较乐观，大部分认为“新冠可防可控”、“传染力有限”，然后钟南山院士对新冠“肯定存在人传人”的表态可能是导致话题量飙升的重要原因。\n",
    "\n",
    "7日晚上李文亮医生不幸去世，微博和朋友圈都在刷屏，从曲线上看，相关话题量几乎到达顶峰。\n",
    "\n",
    "（**悼念李文亮医生**）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正文长度统计\n",
    "\n",
    "现在开始统计相关微博的长度，训练集和测试集都有。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labeled_copy['Chinese_Content_Length'] = train_labeled['Chinese_Content'].astype(str).apply(len)\n",
    "test_copy['Chinese_Content_Length'] = train_labeled['Chinese_Content'].astype(str).apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(14,8))\n",
    "sns.distplot(train_labeled_copy['Chinese_Content_Length'], ax=ax1, color='blue')\n",
    "sns.distplot(test_copy['Chinese_Content_Length'], ax=ax2, color='green')\n",
    "\n",
    "plt.rcParams['font.sans-serif']=['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "ax1.set_title('训练集微博长度分布')\n",
    "ax2.set_title('测试集微博长度分布')\n",
    "\n",
    "plt.show()\n",
    "### 正文词云\n",
    "\n",
    "采用 `jieba` 和 `wordcloud` 对正文做一个词云。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = open('/home/zhw/PycharmProjects/nCovSentimentAnalysis/Docs/cn_stopwords.txt', 'r+', encoding='utf-8')\n",
    "stopword = stop.read().split(\"\\n\")\n",
    "stopeword = set(stopword)\n",
    "stop.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stripword(seg):\n",
    "    \"\"\"停用词处理\"\"\"\n",
    "    wordlist = []\n",
    "    \n",
    "    for key in seg.split(' '):\n",
    "        #去除停用词和单字\n",
    "        if not (key.strip() in stopword) and (len(key.strip()) > 1):\n",
    "            wordlist.append(key)\n",
    "    return ' '.join(wordlist)\n",
    "\n",
    "def cutword(content):\n",
    "    \"\"\"分词，去除停用词，写得比较简陋\"\"\"\n",
    "    seg_list = jieba.cut(content)\n",
    "    line = \" \".join(seg_list)\n",
    "    word = stripword(line)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labeled_copy['Chinese_Content_cut'] = train_labeled['Chinese_Content'].astype(str).apply(cutword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_labeled_copy.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = r'/home/zhw/PycharmProjects/nCovSentimentAnalysis/Docs/MYSH.TTC'\n",
    "wc = WordCloud(font_path=font,\n",
    "               max_words=2000,\n",
    "               width=1800, \n",
    "               height=1600, \n",
    "               mode='RGBA', \n",
    "               background_color=None).generate(str(train_labeled_copy['Chinese_Content_cut'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 12))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 图片统计\n",
    "在训练数据（微博）中，有些是有图片的，有些是没有图片的。\n",
    "\n",
    "我们做一个简单统计："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labeled_copy['Pic_Length'] = train_labeled_copy['Pictures'].apply(lambda x: len(eval(x)))\n",
    "test_copy['Pic_Length'] = test_copy['Pictures'].apply(lambda x: len(eval(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 8))\n",
    "ax1.set_xlim(0, 9)\n",
    "ax2.set_xlim(0, 9)\n",
    "\n",
    "sns.distplot(train_labeled_copy['Pic_Length'], bins=25, ax=ax1, color='blue', kde=False)\n",
    "sns.distplot(test_copy['Pic_Length'], bins=25, ax=ax2, color='green', kde=False)\n",
    "\n",
    "plt.rcParams['font.sans-serif']=['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "ax1.set_title('训练集图片数量分布')\n",
    "ax2.set_title('测试集图片数量分布')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出分布非常近似，这里基本没什么问题。\n",
    "\n",
    "大多数人都是不发图片或者发一张图片。\n",
    "\n",
    "至于9图比7、8图的多，6图比5图多，大概是强迫症..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 视频统计\n",
    "\n",
    "视频计数在训练集和测试集中分布："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labeled_copy['With_Video'] = train_labeled_copy['Videos'].apply(lambda x: len(eval(x)))\n",
    "test_copy['With_Video'] = test_copy['Videos'].apply(lambda x: len(eval(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 8))\n",
    "\n",
    "sns.countplot(train_labeled_copy['With_Video'], ax=ax1, color='grey')\n",
    "sns.countplot(test_copy['With_Video'], ax=ax2, color='orange')\n",
    "\n",
    "plt.rcParams['font.sans-serif']=['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "ax1.set_title('训练集视频分布')\n",
    "ax2.set_title('测试集视频分布')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看起来分布是一致的。\n",
    "\n",
    "接下来我们看看带视频和不带视频的情感标签分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_labeled_copy_2 = train_labeled_copy[(train_labeled_copy[\"Labels\"] == '0') |\n",
    "                                          (train_labeled_copy[\"Labels\"] == '1') |\n",
    "                                          (train_labeled_copy[\"Labels\"] == '-1')]\n",
    "\n",
    "sns.countplot(x='With_Video', hue='Labels', data= train_labeled_copy_2,\n",
    "              order = train_labeled_copy['With_Video'].dropna().value_counts().index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "可以看出，大部分带视频的微博，其情感为中性。\n",
    "\n",
    "但是明显的，相对于不带视频的微博，带视频的微博中 `正面情感` 比例比 `负面情感` 的比例更高。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
